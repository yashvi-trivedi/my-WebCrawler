{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Web Crawler using the Beautiful Soup library"
      ],
      "metadata": {
        "id": "PVM3EklW1Tyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBCIo1UqJnWw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting URL from user input\n",
        "url = input(\"Please enter the URL: \")\n",
        "\n",
        "# Get threshold from user input (with validation)\n",
        "while True:\n",
        "    try:\n",
        "        threshold_token = int(input(\"Please enter the threshold value (integer): \"))\n",
        "        break\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter an integer.\")\n",
        "\n",
        "#saving urls in a file (if the user wishes to do so)\n",
        "passedfile = input(\"Enter the output file name: \")\n",
        "\n",
        "urls=[]\n",
        "\n",
        "pattern = r\"(?P<domain>^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/\\n]+))\"\n",
        "\n",
        "match = re.search(pattern, url)\n",
        "orgdomain = match.group(\"domain\")\n",
        "\n",
        "#function for scraping\n",
        "def scrape_links(url, n=0, maxdepth=threshold_token):\n",
        "    print(n)\n",
        "    if n >= maxdepth:\n",
        "        return\n",
        "    else:\n",
        "        # getting Site data and make it into a BS4 object\n",
        "        webpage = requests.get(url)\n",
        "        html_content = webpage.content\n",
        "        soup = BeautifulSoup(html_content, \"lxml\")\n",
        "\n",
        "        for tag in soup.find_all('a', href=True):\n",
        "            href = tag.get(\"href\")\n",
        "\n",
        "            # using urljoin to create absolute URLs from relative paths\n",
        "            href = urljoin(url, href)\n",
        "\n",
        "            if href.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            if passedfile:\n",
        "                with open(passedfile, 'a') as writer:\n",
        "                    for x in (href):\n",
        "                        writer.write(x)\n",
        "                    writer.write(\"\\n\")\n",
        "            else:\n",
        "                print(\" \" * n * 4 + href)\n",
        "\n",
        "            if href not in urls:\n",
        "                urls.append(href)\n",
        "                scrape_links(href, n + 1, maxdepth)\n",
        "\n",
        "scrape_links(url, 0, threshold_token)"
      ],
      "metadata": {
        "id": "OhGu0XPKN-O7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}